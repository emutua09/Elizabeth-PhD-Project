{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emutua09/Elizabeth-PhD-Project/blob/main/notebooks/prematurity_stage_III_diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ_HbbyC2VuV"
      },
      "source": [
        "## Retinopathy of Prematurity Stage III Diagnosis\n",
        "> Using CNNs for ROP disease diagnosis\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- authors: Elizabeth Mutua, Dr. Bernard Shibwabo & Prof. Christoph Reich\n",
        "- categories: [python, deep learning, computer vision]\n",
        "- image: images/posts/retina.png\n",
        "- cover: images/covers/retina.jpg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6PMFDUN3Ou-"
      },
      "source": [
        "## 1. Overview\n",
        "\n",
        "This PhD research developed an hybrid model combining the use of morphological operations and a Convolutional Neural Network (CNN) for ROP stage III disease diagnosis. Data was obtained from Kaggle database and KNH hospital database.\n",
        "\n",
        "The modeling is performed in PyTorch. All notebooks and a PDF report are available on [Github](https://github.com/emutua09/Elizabeth-PhD-Project)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5UkSnar37lO"
      },
      "source": [
        "## 2. Background\n",
        "\n",
        "### 2.2 Retinopathy of Prematurity (ROP)\n",
        "\n",
        "Retinopathy of Prematurity (ROP) is an eye disease which affects newborn babies born preterm.\n",
        "\n",
        "Babies born preterm, have their retina not fully developed and, in some cases, the blood vessels begins to grow abnormally.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUOpAZS54Wfp"
      },
      "source": [
        "### 2.3 ROP Disease Stages\n",
        "\n",
        "**Stage I:** A whitish demarcation line is formed.\n",
        "\n",
        "**Stage II:** Ridge line increases in width and height\n",
        "\n",
        "**Stage III:** Abnormal growth of blood vessels, Ridgeline turns pink\n",
        "\n",
        "**Stage IV:** Retina Detachment\n",
        "\n",
        "**Stage V:** Blindness\n",
        "\n",
        "![](images/stages.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czS4xN7f6-9u"
      },
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "The data set is available for the download at the [Kaggle database.](https://www.kaggle.com/code/solennollivier/rop-2classclassification/input?select=NewROPDataset_Sample_justtotry)\n",
        "\n",
        "Data had been labelled.\n",
        "\n",
        "Images were randomly selected at a ration of training=0.80, Testing =0.10, Validation =0.10.\n",
        "\n",
        "![](images/Table.png)\n",
        "\n",
        "Let's start by importing the data and looking at the class distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yynVrWMl7XBd"
      },
      "source": [
        "### 3.1 Image Preprocessing\n",
        "\n",
        "#### 3.1.1 Grayscale Conversion\n",
        "\n",
        "The conversion of images into grey scale helped to achieve uniformity of color by converting images of different color shades into grey scale. This process is aimed at increasing accuracy and clear features display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9211gAD_5qD"
      },
      "outputs": [],
      "source": [
        "a=imread(‘image.jpg’);\n",
        "b=rgb2gray(a);\n",
        "imshow(b);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW0gwXLLAI9v"
      },
      "source": [
        "![](images/Grayscare_conversion.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoJswe-lAg8n"
      },
      "source": [
        "#### 3.1.2 Image Enhancement\n",
        "\n",
        "CLAHE an improved version of the adaptive histogram equalization method was applied to reduce noise from the images and enhance the image features for better clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_518FNEApjJ"
      },
      "outputs": [],
      "source": [
        "I=imread('image.jpg');\n",
        "subplot(2,2,1),imshow(I);\n",
        "title('ROPStageIII Original');\n",
        "greenI=I(:,:,2);\n",
        "subplot(1,1,1);\n",
        "imshow(greenI);\n",
        "title(' ROPStageIII  Green channel');\n",
        "clahed = adapthisteq(greenI);\n",
        "subplot(1,1,1),imshow(clahed);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9NWCk2A112"
      },
      "source": [
        "![](images/image_enhancement.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc821GvAA_Ge"
      },
      "source": [
        "#### 3.1.3 Image Feature Extraction\n",
        "\n",
        "Retinal vessel segmentation was done to obtain the images vessels vascular structure, their branching patterns, length, width, and angle. These are key features used for the identification of the disease stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXNvxvS5BIuR"
      },
      "outputs": [],
      "source": [
        "a = imread('image');\n",
        "dim = ndims(a);\n",
        "if(dim == 3)\n",
        "\n",
        "  #convert image into greyscal\n",
        "  a = rgb2gray(a);\n",
        "end\n",
        "\n",
        "#Extracting Blood Vessels\n",
        "Threshold = 10;\n",
        "bloodVessels = VesselExtract(a, Threshold);\n",
        "\n",
        "#Display Blood Vessels image\n",
        "figure;subplot(121);imshow(a);title('Input Image');\n",
        "subplot(122);imshow(bloodVessels);title('Extracted Blood Vessels');\n",
        "function bloodVessels = VesselExtract(a, threshold)\n",
        "\n",
        "#Kirsch's Templates\n",
        "h1=[5 -3 -3;     5  0 -3;     5 -3 -3]/15;\n",
        "h2=[-3 -3 5;     -3  0 5;     -3 -3 5]/15;\n",
        "h3=[-3 -3 -3;    5  0 -3;      5  5 -3]/15;\n",
        "h4=[-3  5  5;     -3  0  5;    -3 -3 -3]/15;\n",
        "h5=[-3 -3 -3;    -3  0 -3;     5  5  5]/15;\n",
        "h6=[ 5  5  5;    -3  0 -3;    -3 -3 -3]/15;\n",
        "h7=[-3 -3 -3;    -3  0  5;    -3  5  5]/15;\n",
        "h8=[ 5  5 -3;     5  0 -3;    -3 -3 -3]/15;\n",
        "\n",
        "#Spatial Filtering by Kirsch's Templates\n",
        "t1=filter2(h1,a); t2=filter2(h2,a);t3=filter2(h3,a);\n",
        "t4=filter2(h4,a);t5=filter2(h5,a);t6=filter2(h6,a);\n",
        "t7=filter2(h7,a);t8=filter2(h8,a);s=size(a);\n",
        "bloodVessels=zeros(s(1),s(2));\n",
        "temp=zeros(1,8);\n",
        "\n",
        "for i=1:s(1)\n",
        "    for j=1:s(2)\n",
        "        temp(1)=t1(i,j);temp(2)=t2(i,j);temp(3)=t3(i,j);temp(4)=t4(i,j);\n",
        "        temp(5)=t5(i,j);temp(6)=t6(i,j);temp(7)=t7(i,j);temp(8)=t8(i,j);\n",
        "        if(max(temp)>threshold)\n",
        "            bloodVessels(i,j)=max(temp);\n",
        "        end\n",
        "    end\n",
        "end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0GISQRvC3_v"
      },
      "source": [
        "![](images/image_feature_extraction.png)\n",
        "\n",
        "Retinopathy of Prematurity stage III is characterized by formation of a ridge line after the blood vessels have stopped growing. The model is developed to extract ridgeline marks from images and is trained to classify the presence of the disease stage using this mark of the ridge line.\n",
        "\n",
        "![](images/image_feature_extraction2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5urnn6qDKoP"
      },
      "source": [
        "## 4. Convolutional Neural Network\n",
        "\n",
        "The model pipeline has three phases of development:\n",
        "- **Training:** -We trained the CNN model on a larger data set from KNH hospital\n",
        "- **Fine-tuning:** We finetuned the model on the target data set. We used cross-validation and made modelling decisions based on the performance of the out-of-fold predictions.\n",
        "- **Inference:** We aggregated predictions of the model trained on different combinations of training folds and applied test-time augmentation to further improve the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7f-hKi7DjFo"
      },
      "source": [
        "### 4.1 Model Training\n",
        "\n",
        "It is complex to build a neural architecture from scratch, we customized an existing CNN architecture build to diagnose Diabetic Retinopathy with data published at [Kaggle competition](https://www.kaggle.com/datasets/solennollivier/ropstages-reviewed) and applied transfer learning to fine tune the model using our data.\n",
        "\n",
        "We initialized the weights from a CNN trained on DR Data and trained it using KNH data and fine-tuned it using a mix of KHN data and Kaggle database data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoxuQMUAD5Zy"
      },
      "source": [
        "#### 4.1.1 Input Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7DwKGaTEuvG"
      },
      "outputs": [],
      "source": [
        "# initial data directory\n",
        "data_dir_nonaug = 'dataRetinopathy'\n",
        "\n",
        "\n",
        "# labeling and resizing all of data that'll be used\n",
        "# 0 = ROPStage2 and 1 = ROPStage3\n",
        "data_nonaug = []\n",
        "categories = [' ROPStage2', ' ROPStage3']\n",
        "img_size = 224*224\n",
        "\n",
        "\n",
        "for category in categories:\n",
        "    path = os.path.join(data_dir_nonaug, category1)\n",
        "    class_num = categories.index(category1);for img in os.listdir(path):\n",
        "        try:\n",
        "          img_array = cv2.imread(os.path.join(path,img));\n",
        "          new_array = cv2.resize(img_array, (img_size, img_size));\n",
        "          data_nonaug.append([new_array, class_num]); except Exception as e:\n",
        "            pass;\n",
        "            print('\\n Jumlah image data sebelum augmentasi: {0}'.format(len(data_nonaug)))\n",
        "\n",
        "# Output: Jumlah image data sebelum augmentasi: 91\n",
        "\n",
        "\n",
        "\n",
        "a_nonaug = []\n",
        "b_nonaug = []\n",
        "\n",
        "if_for features, label in data_nonaug:\n",
        "    a_nonaug.append(features)\n",
        "    b_nonaug.append(label)\n",
        "\n",
        "b_nonaug = np.array(b_nonaug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzlQX7HKFga0"
      },
      "source": [
        "#### Data Argumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulD5KYomFjs5"
      },
      "outputs": [],
      "source": [
        "#os should be imported from tensorflow.keras.utils, along with img_to_array and array_to_img\n",
        "import load_img from keras.preprocessing.image from tensorflow.keras.utils ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=2,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=[0.85, 1.15],\n",
        "    horizontal_flip = True,\n",
        "    fill_mode ='nearest'\n",
        "  )\n",
        "\n",
        "# ROPStage2 augmentation\n",
        "Path_ROPStage2 = os.path.join(data_dir, 'ROPStage2')\n",
        "for image in os.listdir(path_ ROPStage2):\n",
        "  image = load_img('dataRetinopathy/ROPStage2/0').format(img)\n",
        "    x = img_to_array(image)\n",
        "    x = ((1,) + x.shape + x.reshape)\n",
        "\n",
        "  For batch in datagen.flow(x, batch_size=1, save_to_dir='dataRetinopathyaugmented/ROPStage2', save_prefix='ROPStage2', save_format='jpg'), set i = 0 as follows:\n",
        "        If i is greater than 26, then i += 1; break\n",
        "\n",
        "# ROPStage3 augmentation\n",
        "Path_ROPStage3 = os.path.join(data_dir, 'ROPStage3')\n",
        "for image in os.listdir(path_ ROPStage3)\n",
        "    image is same to load_img('dataRETINA/ ROPStage3/0').format(img)\n",
        "    x = img_to_array(image)\n",
        "    x = ((1,) + x.shape + x.reshape)\n",
        "\n",
        "\n",
        "    When i = 0\n",
        "    for a batch in datagen.flow(x, batch_size=1,\n",
        "                                save_to_dir='dataRetinopathyaugmented/ROPStage3',\n",
        "                                save_prefix='ROPStage3', and save_format='jpg'),\n",
        "                                the following occurs:\n",
        "\n",
        "                                If i is more than 20, then i += 1:\n",
        "                                break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5_SiBX2I0wi"
      },
      "source": [
        "#### 4.1.2 Image Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk4Zf290I2ab"
      },
      "outputs": [],
      "source": [
        "new_array = cv2.resize(img_array, (img_size, img_size)) plt; img_size = 224.imshow(new_array)\n",
        "\n",
        "'n img size =', 'new_array.shape', plt.show() print img size =  (224, 224, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9CUp3XoJAOF"
      },
      "source": [
        "![](images/image_resizing.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMQwiHrbJE9x"
      },
      "source": [
        "#### 4.1.3 Image Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKvFbD1cJOw2"
      },
      "outputs": [],
      "source": [
        "# labeling and resizing all of data that'll be used.\n",
        "# 0 = ROPStage2 and 1 = ROPStage3.\n",
        "data = []\n",
        "categories = [' ROPStage2', ' ROPStage3']\n",
        "img_size = 224*224\n",
        "\n",
        "os.path.join(data_dir, category) for category in categories\n",
        "   classes.index(category) = class_num\n",
        "   os.listdir(path), for img:\n",
        "   try:\n",
        "   img_array = os.path.join(path,img),cv2.imread\n",
        "   new_array is equal to data.cv2.resize(img_array, (img_size, img_size)).[New_Array, class_num]\n",
        "   except As an exception:\n",
        "   pass\n",
        "   print('\\n Number of image data that will be used: 0.format(len(data)))\n",
        "\n",
        "#Number of images that will be used: 2124\n",
        "\n",
        "# Set Aug X and Y.\n",
        "X_aug = []\n",
        "y_aug = []\n",
        "\n",
        "Label for features is added using the formulas X_aug.append(label) and Y_aug.append(label).\n",
        "\n",
        "= np.array(y_aug)\n",
        "\n",
        "# Data visualization of augmentation before and after.\n",
        "A_nonaug, B_nonaug, C_aug, and D_aug are all equal to 0.\n",
        "\n",
        "If y_nonaug[i] == 0 for i in range(len(y_nonaug)), then a_nonaug += 1; otherwise, b_nonaug += 1\n",
        "If y_aug[i] == 0 for i in range(len(y_aug)), then c_aug += 1; otherwise, d_aug += 1\n",
        "\n",
        "data_nonaug = np.array([a_nonaug, b_nonaug])\n",
        "data_aug = np.array([c_aug, d_aug])\n",
        "dataplot_aug = [data_nonaug, data_aug]\n",
        "jumlah_ROPStage3_aug = [a_nonaug, c_aug]\n",
        "jumlah_ROPStage2_aug = [b_nonaug, d_aug]\n",
        "split_aug = ['SEBELUM', 'SETELAH']\n",
        "\n",
        "for i in range(len(dataplot_aug)):\n",
        "    positions = np.arange(2)\n",
        "    plt.bar(positions, dataplot_aug[i], 0.8)\n",
        "    plt.xticks(positions + 0.05, ('ROPStage 3', 'ROPStage2'))\n",
        "    plt.title('Distribusi Data ROPStage3 dan Ro Pada Dataset {} Augmentasi'.format(split_aug[i]))\n",
        "    print('---Dataset {0} Augmentasi---'.format(split[i]))\n",
        "    plt.show()\n",
        "    print('\\n Jumlah data ROPStage3 pada dataset {0} augmentasi: {1}'.format(split_aug[i], jumlah_ROPStage3_aug[i]))\n",
        "    print('\\n Jumlah data ROPStage2 pada dataset {0} augmentasi: {1}'.format(split_aug[i], jumlah_ ROPStage2_aug[i]))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gb5mHk0LeN0"
      },
      "source": [
        "#### 4.1.4 Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyt1c6AgLtkU"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.80\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# determining the ratio of train-val-test data\n",
        "train-val-test data using val_ratio = 0.10 and test_ratio = 0.10.\n",
        "\n",
        "#split data into:\n",
        "Train_test_split = X_train, X_test, Y_train, and Y_test(X, y, train_ratio 1, test_size 1, random_state 1)\n",
        "X_val, X_test, y_val, and y_test are all equal to train_test_split(X_test, y_test, test_size=(test_ratio + val_ratio), random_state=1)\n",
        "\n",
        "# Visualization of data distribution.\n",
        "a = 0; b = 0; c = 0; d = 0; e = 0; f = 0\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == 0:\n",
        "    a += 1\n",
        "  else:\n",
        "    b += 1\n",
        "\n",
        "for i in range(len(y_val)):\n",
        "  if y_val[i] == 0:\n",
        "    c += 1\n",
        "  else:\n",
        "    d += 1\n",
        "\n",
        "data1 = np.array([a, b])\n",
        "data2 = np.array([c, d])\n",
        "data3 = np.array([e, f])\n",
        "dataplot = [data1, data2, data3]\n",
        "\n",
        "jumlah_ROPStage3 = [a, c, e]\n",
        "jumlah_ROPStage2 = [b, d, f]\n",
        "\n",
        "split = ['Training', 'Validation', 'Test']\n",
        "\n",
        "for i in range(len(dataplot)):\n",
        "    positions = np.arange(2)\n",
        "    plt.bar(positions, dataplot[i], 0.8)\n",
        "    plt.xticks(positions + 0.05, ('ROPStage3', 'ROPStage2'))\n",
        "    plt.title('Distribusi Data ROPStage3 dan ROPStage2 Pada {} Dataset'.format(split[i]))\n",
        "    print('---{0} Dataset---'.format(split[i]))\n",
        "    plt.show()\n",
        "    print('\\n Jumlah data ROPStage3 pada {0} dataset: {1}'.format(split[i], jumlah_ROPStage3 [i]))\n",
        "    print('\\n Jumlah data ROPStage2 pada {0} dataset: {1}'.format(split[i], jumlah_ROPStage2 [i]))\n",
        "    print('\\n')\n",
        "\n",
        "#Jumlah data ROPStage3 pada Training dataset: 1464\n",
        "#Jumlah data ROPStage2 pada Training dataset: 1232\n",
        "#Jumlah data ROPStage3 pada Validation dataset: 183\n",
        "#Jumlah data ROPStage2 pada Validation dataset: 154\n",
        "#Jumlah data ROPStage3 pada Test dataset: 183\n",
        "#Jumlah data ROPStage2 pada Test dataset: 154"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z7roTcVNeDT"
      },
      "source": [
        "#### 4.1.5 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1BVNAx6NhLa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "monitor_val_acc = EarlyStopping(monitor = 'val_accuracy', patience = 3);\n",
        "\n",
        "# callbacks for model\n",
        "save_best_only = True,\n",
        "model_checkpoint = ModelCheckpoint('best_ROPStage3_model.hdf5')\n",
        "\n",
        "# optimizer.\n",
        "opt = Adam(learning_rate=2*(10**(-5)))\n",
        "\n",
        "Model = Sequential();\n",
        "\n",
        "#Model blocks\n",
        "Model Block 1 is.Model: add(Conv2D(64, (3, 3), activation = \"relu\", input_shape = X.shape[1:], padding = \"same\").Addition: Conv2D(64, (3,3), activation = \"relu,\" padding = \"same\"\n",
        "MaxPooling2D (pool_size = (2, 2), strides = 2, padding = \"same\") model.add\n",
        "Model Block 2 is.Addition: Conv2D(128, (3,3), activation = \"relu,\" padding = \"same\"\n",
        "Conv2D(128, (3,3), activation = \"relu,\" padding = \"same\"); model.add\n",
        "model.Pool_size = (2,2), Strides = 2, Padding = \"Same,\" MaxPooling2D, add\n",
        "Model Block 3 is.Model: add(Conv2D(256, (3,3), activation = \"relu,\" padding = \"same\").Model: add(Conv2D(256, (3,3), activation = \"relu,\" padding = \"same\").Model: add(Conv2D(256, (3,3), activation = \"relu,\" padding = \"same\").Model: add(Conv2D(256, (3,3), activation = \"relu,\" padding = \"same\").Addition: MaxPooling2D (pool_size = (2,2), strides = 2)\n",
        "Block 4 model is #.Model: add(Conv2D(512, (3,3), activation = \"relu,\" padding = \"same\").Addition: Conv2D(512, (3,3), activation = \"relu,\" padding = \"same\"\n",
        "Conv2D(512, (3,3), activation = \"relu,\" padding = \"same\"); model.add\n",
        "model.Addition: Conv2D(512, (3,3), activation = \"relu,\" padding = \"same\"\n",
        "model.Pool_size = (2,2), Strides = 2, Padding = \"Same,\" MaxPooling2D, add\n",
        "Model Block 5 is.model = add(Conv2D(512, 3, 3, activation = \"relu\", padding = \"same\")).model = add(Conv2D(512, 3, 3, activation = \"relu\", padding = \"same\")).model = add(Conv2D(512, 3, 3, activation = \"relu\", padding = \"same\")).model = add(Conv2D(512, 3, 3, activation = \"relu\", padding = \"same\")).Pool_size = (2,2), Strides = 2, Padding = \"Same,\" MaxPooling2D, add\n",
        "Model with Fully Connected Layers.add(Flatten()) model.Dense(4096, activation = \"relu\") model added.add(Dropout(0.5)) model.Dense(4096, activation = \"relu\") model added.add(Dropout(0.5)) model.Dense(1000, activation = \"relu\") model added.Dense(2, activation = \"softmax\") added\n",
        "Compiling the model with the following parameters: optimizer = opt, loss = \"sparse_categorical_crossentropy,\" metrics = [\"accuracy\"]\n",
        "\n",
        "\n",
        "# Model Training\n",
        "History = model.fit(X_train, Y_train, epochs = 20, batch_size = 32, validation_data=(X_val, Y_val), callbacks = [monitor_val_acc, model_checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71MLyn8mQC-q"
      },
      "source": [
        "- Epoch 1/20\n",
        "54/54 [==============================] 1044s 19s/step, 0.6927 loss, 0.5009 accuracy, 0.6900 val_loss, and 0.5094 val_accuracy;\n",
        "\n",
        "- Epoch 2/20\n",
        "54/54 [==============================] Loss: 0.6873 Accuracy: 0.5338 750s 14s/step Val_loss: 0.6874 Val_accuracy: 0.5094\n",
        "\n",
        "- Epoch 3/20\n",
        "54/54 [==============================] 4093 steps at 77 seconds each, with a loss of 0.6714 and an accuracy of 0.5974.\n",
        "\n",
        "- Epoch 4/20\n",
        "54/54 [==============================] Loss: 0.6648 Accuracy: 0.6057 Time: 10405s Val_loss: 0.6179 Val_accuracy: 0.6651\n",
        "\n",
        "- Epoch 5/20\n",
        "54/54 [==============================]\n",
        "\n",
        "- Epoch 6/20\n",
        "54/54 - 6564s 123s/step - loss 0.6185 - accuracy 0.6516 - val_loss 0.5469 - val_accuracy 0.7123 [==============================]\n",
        "\n",
        "- Epoch 7/20\n",
        "54/54 - 815s 15s/step - loss 0.5765 - accuracy 0.6869 - val_loss 0.5652 - val_accuracy 0.6792 [==============================] Loss: 0.5017 Accuracy: 0.7387 Val_loss: 0.4230 Val_accuracy: 0.7547 754s 14s/step\n",
        "\n",
        "- Epoch 8/20\n",
        "54/54 [==============================] Epoch 9/20: 761s 14s/step, loss 0.4556, accuracy 0.7569, val_loss 0.4361, val_accuracy 0.7972\n",
        "54/54 [==============================] Loss: 0.4069 Accuracy: 0.7899 Loss: 0.3187 Accuracy: 0.8019 767s 14s/step\n",
        "\n",
        "- Epoch 10/20\n",
        "54/54 [==============================] 821s 15s/step, 0.3456 loss, 0.8228 accuracy, 0.3595 value loss, and 0.8538 value accuracy\n",
        "\n",
        "- Epoch 11/20\n",
        "54/54 [==============================] 863 steps at 16 seconds each step with a loss of 0.3564 and an accuracy of 0.8122.\n",
        "\n",
        "- Epoch 12/20\n",
        "54/54 [==============================] Loss: 0.3220 Accuracy: 0.8370 Val_loss: 0.2624 Val_accuracy: 0.8491 840s 16s/step\n",
        "\n",
        "- Epoch 13/20\n",
        "54/54 [==============================] 821s 15s/step, 0.2696 loss, 0.8664 accuracy, 0.2464 value loss, and 0.8443 value accuracy\n",
        "\n",
        "- Epoch 14/20\n",
        "54/54 [==============================] Loss: 0.2160 Accuracy: 0.9064 Val_loss: 0.2583 Val_accuracy: 0.9057 819s 15s/step\n",
        "\n",
        "- Epoch 15/20\n",
        "54/54 [==============================] Epoch 16/20 54/54 - 876s 16s/step - loss 0.2232 - accuracy 0.8994 - val_loss 0.1843 - val_accuracy 0.9292 [==============================] Loss: 0.1892 Accuracy: 0.9164 Val_loss: 0.1792 Val_accuracy: 0.9340 860s 16s/step\n",
        "\n",
        "- Epoch 17/20\n",
        "54/54 [==============================] 792s 15s/step, 0.1469 loss, 0.9382 accuracy, 0.1387 val_loss, and 0.9481 val_accuracy\n",
        "\n",
        "- Epoch 18/20\n",
        "54/54 [==============================] Loss: 0.1350 Accuracy: 0.9488 Val_loss: 0.1519 Val_accuracy: 0.9340 786s 15s/step\n",
        "\n",
        "- Epoch 19/20\n",
        "54/54 [==============================] Loss: 0.0991 Accuracy: 0.9635 Val_loss: 0.1136 Val_accuracy: 0.9528 797s 15s/step\n",
        "\n",
        "- Epoch 20/20\n",
        "54/54 [==============================] 786s 15s/step, 0.1373 loss, 0.9482 accuracy, 0.1201 val_loss, and 0.9670 val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIL3exyQNmI"
      },
      "source": [
        "#### 4.1.6 Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO5UZBFsQeWT"
      },
      "outputs": [],
      "source": [
        "sns import seaborn\n",
        "import confusion_matrix from sklearn.metrics into matplotlib.pyplot as a plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Summary of the model: model.summary()\n",
        "\n",
        "\n",
        "\"sequential_1\" is the model.\n",
        "\n",
        "\n",
        " Convolutional Layer,              Structure,              Params #\n",
        " =================================================================\n",
        " =================================================================\n",
        " \n",
        " conv2d_1 (Conv2D)          (None, 224, 224, 64)      1792      \n",
        "                                                                 \n",
        " conv2d_2 (Conv2D)          (None, 224, 224, 64)      36928     \n",
        "                                                                 \n",
        " max_pooling2d_1 (MaxPooling  (None, 112, 112, 64)     0         \n",
        " 2D)                                                             \n",
        "                                                              \n",
        " conv2d_3 (Conv2D)          (None, 112, 112, 128)     73856     \n",
        "                                                                 \n",
        " conv2d_4 (Conv2D)          (None, 112, 112, 128)     147584    \n",
        "                                                                 \n",
        " max_pooling2d_2 (MaxPooling  (None, 56, 56, 128)      0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " conv2d_5 (Conv2D)          (None, 56, 56, 256)       295168\n",
        "conv2d_6 (Conv2D)          (None, 56, 56, 256)       590080    \n",
        "                                                                 \n",
        " conv2d_7 (Conv2D)          (None, 56, 56, 256)       590080    \n",
        "                                                                 \n",
        " conv2d_8 (Conv2D)          (None, 56, 56, 256)       590080    \n",
        "                                                                 \n",
        " max_pooling2d_3 (MaxPooling  (None, 28, 28, 256)      0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " conv2d_9 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
        "                                                                 \n",
        " conv2d_10 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
        "                                                                 \n",
        " conv2d_11 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
        "                                                                 \n",
        " conv2d_12 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
        "                                                                 \n",
        " max_pooling2d_4 (MaxPooling  (None, 14, 14, 512)      0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " conv2d_13 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
        "                                                                 \n",
        " conv2d_14 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
        "                                                                 \n",
        " conv2d_15 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
        "                                                                 \n",
        " conv2d_16 (Conv2D)          (None, 14, 14, 512)       2359808\n",
        " max_pooling2d_5 (MaxPooling  (None, 7, 7, 512)        0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " flatten_1a (Flatten)         (None, 25088)             0         \n",
        "                                                                 \n",
        " dense_1 (Dense)             (None, 4096)              102764544\n",
        "                                                                 \n",
        " dropout_1 (Dropout)         (None, 4096)              0         \n",
        "                                                                 \n",
        " dense_2 (Dense)             (None, 4096)              16781312  \n",
        "                                                                 \n",
        " dropout_2 (Dropout)         (None, 4096)              0         \n",
        "                                                                 \n",
        " dense_3 (Dense)             (None, 1000)              4097000   \n",
        "                                                                 \n",
        " dense_4 (Dense)             (None, 2)                 2002      \n",
        "                                                                 \n",
        "=================================================================\n",
        "\n",
        "Param Summary: 143,669,242\n",
        "\n",
        "Trainable: 143,669,242\n",
        "\n",
        "Non-trainable: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eiaZFu3R_5B"
      },
      "source": [
        "\n",
        "![](images/model_accuracy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7rqEsuGS2pV"
      },
      "source": [
        "![](images/model_loss.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
